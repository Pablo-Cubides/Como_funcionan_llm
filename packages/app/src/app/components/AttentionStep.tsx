'use client';

import { useEffect, useState, useMemo } from 'react';
import { useProcess } from '../../context/ProcessContext';

interface AttentionStepProps { onNext?: () => void }
export default function AttentionStep({ onNext }: AttentionStepProps) {
  const { state, dispatch } = useProcess();
  const { processData, isExplanationMode } = state;
  const [hoveredCell, setHoveredCell] = useState<{row: number, col: number} | null>(null);
  const [selectedHead, setSelectedHead] = useState(0);

    // snapshot embeddings to a stable reference for hook dependencies
    const embeddingsSnapshot = useMemo(() => processData?.combinedEmbeddings ?? null, [processData?.combinedEmbeddings]);

    // (combinedKey removed) embeddingsSnapshot is used for effect dependencies

    useEffect(() => {
      if (embeddingsSnapshot && processData && !processData.attentionHeads?.length) {
        dispatch({ type: 'COMPUTE_ATTENTION', payload: { numHeads: 4 } });
      }
    }, [embeddingsSnapshot, processData, dispatch]);

  const getAttentionColor = (weight: number): string => {
    if (weight === 0) return 'bg-slate-800';
    const alpha = Math.pow(weight, 0.5);
    return `rgba(59, 130, 246, ${alpha})`;
  };

  const attentionData = useMemo(() => {
    if (!processData?.attentionHeads?.length) return [];
    if (selectedHead === -1) {
      return processData.attentionWeights || [];
    }
    return processData.attentionHeads[selectedHead]?.weights || [];
  }, [processData?.attentionHeads, processData?.attentionWeights, selectedHead]);

  if (!processData?.attentionHeads?.length) {
    return <div className="p-8 sm:p-12 flex justify-center items-center min-h-[400px]">
      <div className="text-xl text-slate-400">Calculando AtenciÃ³n...</div>
    </div>;
  }

  return (
    <div className="p-8 sm:p-12 panel">
      <div className="text-center mb-12">
        <h2 className="step-title">ğŸ§  Â¿CÃ³mo funciona Self-Attention?</h2>
        {isExplanationMode && (
          <p className="step-description">
            ğŸ§  <strong>Imagina que cada palabra puede &quot;mirar&quot; a todas las demÃ¡s palabras para entenderlas mejor.</strong> 
            Es como cuando lees una frase: tu cerebro conecta automÃ¡ticamente las palabras relacionadas. 
            El modelo hace esto con <strong>4 &quot;cerebros pequeÃ±os&quot;</strong> (cabezas) trabajando al mismo tiempo, 
            cada uno buscando diferentes tipos de conexiones entre las palabras.
          </p>
        )}
      </div>

  <div className="bg-gradient-to-br from-slate-900/60 to-slate-800/40 rounded-2xl border border-slate-700 p-8 shadow-xl">
        <div className="flex flex-col md:flex-row justify-between items-start md:items-center gap-4 mb-6">
            <h3 className="text-2xl font-bold text-slate-200">Matriz de Pesos de AtenciÃ³n</h3>
            <div className="flex items-center gap-3">
                <label htmlFor="head-selector" className="text-sm font-semibold text-slate-300">Seleccionar Cabeza:</label>
                <select 
                    id="head-selector"
                    value={selectedHead}
                    onChange={(e) => setSelectedHead(Number(e.target.value))}
                    className="bg-slate-800 text-white text-base font-medium rounded-xl px-4 py-2.5 border-2 border-slate-600 focus:ring-2 focus:ring-blue-500 focus:border-blue-500 focus:outline-none transition-all cursor-pointer">
                    <option value={-1}>ğŸ”— Vista Combinada (todas)</option>
                    {processData.attentionHeads.map((_, i) => (
                        <option key={i} value={i}>ğŸ¯ Cabeza {i + 1}</option>
                    ))}
                </select>
            </div>
        </div>
        <div className="overflow-x-auto">
          <div className="inline-block min-w-full align-middle">
            <table className="w-full border-separate border-spacing-1">
              <thead>
                <tr>
                  <th className="sticky left-0 bg-slate-900/75 backdrop-blur-sm"></th>
                  {processData.tokens.map((token, index) => (
                    <th key={index} className="text-center font-medium text-slate-400 p-2">
                      {token === ' ' ? 'â£' : token}
                    </th>
                  ))}
                </tr>
              </thead>
              <tbody>
                {attentionData.map((row, rowIndex) => (
                  <tr key={rowIndex}>
                    <th className="sticky left-0 bg-slate-900/75 backdrop-blur-sm text-right font-medium text-slate-400 p-2">
                      {processData.tokens[rowIndex] === ' ' ? 'â£' : processData.tokens[rowIndex]}
                    </th>
                    {row.map((weight, colIndex) => (
                      <td
                        key={colIndex}
                        className="relative rounded-md transition-all duration-150 ease-in-out"
                        style={{ backgroundColor: getAttentionColor(weight) }}
                        onMouseEnter={() => setHoveredCell({row: rowIndex, col: colIndex})}
                        onMouseLeave={() => setHoveredCell(null)}
                      >
                        <div className="w-16 h-16 flex items-center justify-center text-sm font-bold text-white/90">
                          {(weight * 100).toFixed(0)}%
                        </div>
                        {hoveredCell?.row === rowIndex && hoveredCell?.col === colIndex && (
                            <div className="absolute bottom-full mb-2 left-1/2 -translate-x-1/2 px-3 py-1.5 bg-slate-950 text-white text-xs rounded-md shadow-lg z-10 whitespace-nowrap border border-slate-600">
                                {processData.tokens[rowIndex]} â†’ {processData.tokens[colIndex]}: {(weight * 100).toFixed(1)}%
                            </div>
                        )}
                      </td>
                    ))}
                  </tr>
                ))}
              </tbody>
            </table>
          </div>
        </div>
    {isExplanationMode && (
      <div className="mt-6 p-6 bg-gradient-to-br from-purple-950/6 to-slate-900/20 rounded-2xl">
        {/* General explanation visible first */}
        <div className="mb-4 text-slate-300">
          <h4 className="font-bold text-2xl text-purple-300 mb-2 flex items-center gap-2">ğŸ§  Â¿CÃ³mo funciona Self-Attention?</h4>
              <p className="text-sm leading-relaxed">
                Self-attention es la forma en que el modelo decide quÃ© partes de la frase son importantes para cada palabra. Cada palabra &ldquo;mira&rdquo; a todas las demÃ¡s palabras y les asigna un peso de importancia (atenciÃ³n). Las mÃ¡s relevantes reciben puntajes altos; las menos relevantes, puntajes bajos. Esta es la base de los Transformers y de casi todos los LLMs modernos.
              </p>
          <h5 className="font-semibold text-lg text-slate-200 mt-4">ğŸ“Š CÃ³mo leer la matriz de atenciÃ³n</h5>
          <ul className="list-disc list-inside text-slate-300 text-sm mt-2 space-y-2">
            <li>Cada fila representa una palabra que estÃ¡ &ldquo;prestando atenciÃ³n&rdquo;.</li>
            <li>Cada columna representa una palabra que podrÃ­a ser importante para ella.</li>
            <li>El nÃºmero (porcentaje) indica cuÃ¡nta atenciÃ³n le dedica una palabra a otra.</li>
            <li>100% â†’ sÃºper importante en ese momento. 0% â†’ prÃ¡cticamente ignorada. El color mÃ¡s oscuro significa &ldquo;mÃ¡s atenciÃ³n&rdquo;.</li>
          </ul>
          <p className="text-sm leading-relaxed mt-3">Ejemplo de lectura: si en la fila de una palabra ves porcentajes altos sobre otra palabra, significa &ldquo;esta palabra necesita a esa otra para entender su propio significado en contexto&rdquo;.</p>
          <h5 className="font-semibold text-lg text-slate-200 mt-4">ï¿½ Â¿Por quÃ© hace eso?</h5>
          <p className="text-sm leading-relaxed">Porque entender una frase no es solo leer palabras aisladas. El modelo necesita saber quiÃ©n hace la acciÃ³n, quÃ© acciÃ³n ocurre, a quÃ© se refiere esa acciÃ³n y con quÃ© matiz. Self-attention permite que el modelo aprenda todas esas relaciones en paralelo, capturando contexto largo y dependencias complejas.</p>
        </div>

        {/* Detailed accordion */}
        <details className="bg-slate-900/50 rounded-2xl border border-slate-700 p-4">
            <summary className="cursor-pointer font-bold text-lg text-purple-300">ğŸ” ExplicaciÃ³n Detallada (haz clic para expandir)</summary>
          <div className="mt-4 text-slate-300 space-y-4 text-sm">
            <h5 className="font-semibold">ğŸ’¡ Resumen rÃ¡pido</h5>
            <p>Self-attention responde: Â¿A quÃ© otras palabras deberÃ­a mirar esta palabra para entender su rol? Â¿QuiÃ©n hace quÃ©? Â¿Sobre quÃ©? Â¿En quÃ© contexto? Los nÃºmeros de la matriz indican cuÃ¡nta atenciÃ³n se presta a cada otra palabra; eso se calcula matemÃ¡ticamente.</p>

            <h5 className="font-semibold">âš  Mito vs Realidad</h5>
            <p><strong>âš  Mito:</strong> &ldquo;El modelo solo lee palabra por palabra, como nosotros en voz alta.&rdquo;</p>
            <p><strong>âœ… Realidad:</strong> &ldquo;El modelo conecta todas las palabras entre sÃ­ en paralelo y les asigna pesos de importancia. Eso es self-attention.&rdquo;</p>

            <h5 className="font-semibold">ğŸ§  Q / K / V (Query, Key, Value)</h5>
            <p>Para cada palabra el modelo genera tres vectores distintos:</p>
            <ul className="list-disc list-inside">
              <li><strong>ğŸ” Query (Q)</strong> = &ldquo;lo que estoy buscando&rdquo;</li>
              <li><strong>ğŸ”‘ Key (K)</strong> = &ldquo;quÃ© ofrezco&rdquo; (tarjeta de identificaciÃ³n)</li>
              <li><strong>ğŸ“¦ Value (V)</strong> = &ldquo;mi contenido Ãºtil&rdquo;</li>
            </ul>
            <p>Flujo: comparamos Query de una palabra con los Keys de TODAS las palabras â†’ obtenemos pesos â†’ hacemos un promedio ponderado de los Values.</p>

            <h5 className="font-semibold">ğŸ”¢ FÃ³rmula</h5>
            <p className="font-mono">Attention = Softmax(Q Â· K<sup>T</sup> / âˆšd<sub>k</sub>) Â· V</p>
            <p>Q Â· K<sup>T</sup> = quÃ© tan bien encaja lo que busco (Q) con lo que cada palabra ofrece (K). âˆšd<sub>k</sub> = factor de escala. Softmax = convierte esos puntajes en porcentajes. Multiplicar por V mezcla la informaciÃ³n de las palabras mÃ¡s relevantes.</p>

            <h5 className="font-semibold">ğŸ‘¥ Â¿Por quÃ© varias cabezas?</h5>
            <p>No usamos una sola matriz sino varias en paralelo (multi-head). Cada cabeza se enfoca en diferentes relaciones (sujeto-verbo, verbo-objeto, modificadores, puntuaciÃ³n). Luego se combinan para una comprensiÃ³n mÃ¡s rica. Modelos grandes usan muchas cabezas.</p>

            <h5 className="font-semibold">ğŸ“Œ ConexiÃ³n con el resto del modelo</h5>
            <p>El resultado de self-attention es una nueva versiÃ³n de cada palabra contextualizada; esa representaciÃ³n pasa a las siguientes capas y finalmente se usa para predecir la siguiente palabra con una distribuciÃ³n de probabilidad.</p>
          <h5 className="font-semibold text-lg text-slate-200 mt-4">ğŸ¯ Â¿QuÃ© es una &ldquo;cabeza de atenciÃ³n&rdquo;?</h5>
          <p className="text-sm leading-relaxed">El modelo usa varias &ldquo;cabezas de atenciÃ³n&rdquo; al mismo tiempo. Cada cabeza actÃºa como un lector distinto: una puede enfocarse en sujeto â†” verbo, otra en objeto, otra en matices, etc. En el selector de &quot;Cabeza&quot; puedes ver cada una de esas perspectivas por separado.</p>
          <p className="text-sm leading-relaxed mt-2">El resultado de self-attention es una versiÃ³n contextualizada de cada palabra. DespuÃ©s de este paso, el modelo ya no ve palabras aisladas, sino ideas conectadas en contexto; esa representaciÃ³n contextual se usa en el siguiente paso para calcular probabilidades.</p>
            <h5 className="font-semibold">ğŸ“š BibliografÃ­a / Lecturas recomendadas</h5>
            <ul className="list-disc list-inside text-slate-400">
              <li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer" className="text-blue-300 underline">Vaswani, A. et al. (2017). Attention Is All You Need</a></li>
              <li><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noreferrer" className="text-blue-300 underline">Devlin, J. et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers</a></li>
              <li><a href="https://arxiv.org/abs/1508.07909" target="_blank" rel="noreferrer" className="text-blue-300 underline">Sennrich, R. et al. (2016). Neural Machine Translation of Rare Words with Subword Units</a></li>
            </ul>
          </div>
        </details>
      </div>
    )}
      </div>

      <div className="text-center mt-8">
        <button className="navigation-button px-8 py-3" onClick={() => onNext ? onNext() : null}>
          <span>Siguiente: Probabilidades</span>
          <span>â†’</span>
        </button>
      </div>
    </div>
  );
}
